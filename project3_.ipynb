{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 620 - Week5 Group Project #3\n",
    "\n",
    "# Team Members\n",
    "* # Ali Ahmed, Nilsa Bermudez, Ursula Podosenin\n",
    "\n",
    "\n",
    "# Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:35:15.315992Z",
     "iopub.status.busy": "2025-06-19T23:35:15.315734Z",
     "iopub.status.idle": "2025-06-19T23:35:15.480441Z",
     "shell.execute_reply": "2025-06-19T23:35:15.479402Z",
     "shell.execute_reply.started": "2025-06-19T23:35:15.315972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Labeling names with gender\n",
    "labeled_names = [(name, 'male') for name in names.words('male.txt')] + \\\n",
    "                [(name, 'female') for name in names.words('female.txt')]\n",
    "\n",
    "# Shuffling the dataset\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "# Splitting the data into training (6900), dev-test (500), and test (500)\n",
    "train_set = labeled_names[1000:]\n",
    "devtest_set = labeled_names[500:1000]\n",
    "test_set = labeled_names[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:35:15.481639Z",
     "iopub.status.busy": "2025-06-19T23:35:15.481370Z",
     "iopub.status.idle": "2025-06-19T23:35:15.501143Z",
     "shell.execute_reply": "2025-06-19T23:35:15.499830Z",
     "shell.execute_reply.started": "2025-06-19T23:35:15.481619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    features = {\n",
    "        'first_letter': word[0].lower(),\n",
    "        'last_letter': word[-1].lower(),\n",
    "        'last_two': word[-2:].lower(),\n",
    "        'last_is_vowel': word[-1].lower() in 'aeiou',\n",
    "        'name_length': len(word),\n",
    "    }\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[f'count_{letter}'] = word.lower().count(letter)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:35:15.502795Z",
     "iopub.status.busy": "2025-06-19T23:35:15.502473Z",
     "iopub.status.idle": "2025-06-19T23:35:15.549656Z",
     "shell.execute_reply": "2025-06-19T23:35:15.548686Z",
     "shell.execute_reply.started": "2025-06-19T23:35:15.502768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_feats = [(gender_features(n), g) for (n, g) in train_set]\n",
    "devtest_feats = [(gender_features(n), g) for (n, g) in devtest_set]\n",
    "test_feats = [(gender_features(n), g) for (n, g) in test_set]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:35:15.551951Z",
     "iopub.status.busy": "2025-06-19T23:35:15.551662Z",
     "iopub.status.idle": "2025-06-19T23:35:15.664867Z",
     "shell.execute_reply": "2025-06-19T23:35:15.664085Z",
     "shell.execute_reply.started": "2025-06-19T23:35:15.551927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev-Test Accuracy: 0.784\n",
      "Test Accuracy: 0.782\n",
      "Most Informative Features\n",
      "                last_two = 'na'           female : male   =     93.7 : 1.0\n",
      "                last_two = 'la'           female : male   =     71.4 : 1.0\n",
      "             last_letter = 'k'              male : female =     38.4 : 1.0\n",
      "                last_two = 'ia'           female : male   =     37.6 : 1.0\n",
      "             last_letter = 'a'            female : male   =     37.2 : 1.0\n",
      "                last_two = 'ra'           female : male   =     33.2 : 1.0\n",
      "                last_two = 'io'             male : female =     26.1 : 1.0\n",
      "                last_two = 'us'             male : female =     26.0 : 1.0\n",
      "                last_two = 'rd'             male : female =     23.9 : 1.0\n",
      "                last_two = 'ta'           female : male   =     23.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on dev-test and test sets\n",
    "print(\"Dev-Test Accuracy:\", nltk.classify.accuracy(classifier, devtest_feats))\n",
    "print(\"Test Accuracy:\", nltk.classify.accuracy(classifier, test_feats))\n",
    "\n",
    "# Show informative features\n",
    "classifier.show_most_informative_features(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T23:35:15.666133Z",
     "iopub.status.busy": "2025-06-19T23:35:15.665847Z",
     "iopub.status.idle": "2025-06-19T23:35:17.148892Z",
     "shell.execute_reply": "2025-06-19T23:35:17.148047Z",
     "shell.execute_reply.started": "2025-06-19T23:35:15.666109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some misclassified names:\n",
      "Name: Amabel               True: female  Predicted: male\n",
      "Name: Brook                True: female  Predicted: male\n",
      "Name: Darb                 True: female  Predicted: male\n",
      "Name: Gael                 True: female  Predicted: male\n",
      "Name: Lynnett              True: female  Predicted: male\n",
      "Name: Margaux              True: female  Predicted: male\n",
      "Name: Ricky                True: female  Predicted: male\n",
      "Name: Shelley              True: female  Predicted: male\n",
      "Name: Shirleen             True: female  Predicted: male\n",
      "Name: Ambrose              True: male    Predicted: female\n"
     ]
    }
   ],
   "source": [
    "# Misclassified names in the dev-test set\n",
    "errors = []\n",
    "for (name, gender) in devtest_set:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != gender:\n",
    "        errors.append((gender, guess, name))\n",
    "\n",
    "print(\"\\nSome misclassified names:\")\n",
    "for (gender, guess, name) in sorted(errors[:10]):  # just show 10 for brevity\n",
    "    print(f\"Name: {name:<20} True: {gender:<7} Predicted: {guess}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation and Comparison\n",
    "\n",
    "After evaluating our final classifier, we found:\n",
    "\n",
    "- Dev-test Set Accuracy: 78.8%\n",
    "- Test Set Accuracy: 75.8%\n",
    "\n",
    "The accuracy on the test set is very close to the dev-test accuracy, which is expected. This small difference suggests that the classifier generalizes well to unseen data and has not overfitted the dev-test set.\n",
    "\n",
    "A small drop in test accuracy is common and acceptable. If the test set accuracy had dropped significantly, it would indicate overfitting. The consistent performance here confirms that the features selected (such as last letter, name length, vowel/consonant counts) are useful and general.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Upon inspecting the misclassified names, we notice that some names like \"Amadel\" or \"Cyb\" are gender-ambiguous and could belong to either category. These examples highlight the limitation of using only name-based features and suggest potential value in adding external features.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
